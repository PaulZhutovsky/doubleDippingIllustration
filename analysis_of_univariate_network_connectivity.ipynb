{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score\n",
    "from sklearn.feature_selection import SelectFpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = '/data/shared/ptsdchild'\n",
    "analysis_folder = osp.join(project_folder, 'analysis', 'resting_state', 'patients_ptsdchild_maps_dim70.gigica')\n",
    "ml_folder = osp.join(analysis_folder, 'ml_analysis')\n",
    "between_folder = osp.join(analysis_folder, 'group_comparison', 'between_networks')\n",
    "\n",
    "# Random seed used in the actual analysis: used here for consistency\n",
    "random_state = 1588353789\n",
    "\n",
    "n_repeats, n_splits = 50, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_subj: 40; n_connections: 1128\n",
      "n_significant: 1: p_FWE: 0.012299999999999978\n",
      "(array([0., 1.]), array([19, 21]))\n"
     ]
    }
   ],
   "source": [
    "# p_values are stored as 1-p\n",
    "alpha = 0.95\n",
    "\n",
    "df_correlations = pd.read_csv(osp.join(between_folder, 'unique_correlation_matrix.csv'), header=None)\n",
    "print('n_subj: {}; n_connections: {}'.format(*df_correlations.shape))\n",
    "df_result = pd.read_csv(osp.join(between_folder, \n",
    "                                 'palm_responder_vs_nonresponder_30perc_dat_tstat_mcfwep_m1_c2.csv'),\n",
    "                        header=None)\n",
    "result = df_result.to_numpy().squeeze()\n",
    "id_significant = result > alpha\n",
    "p_max = 1 - result[id_significant][0]\n",
    "print('n_significant: {}: p_FWE: {}'.format(id_significant.sum(), p_max))\n",
    "significant_connection = df_correlations.to_numpy()[:, id_significant].squeeze()\n",
    "\n",
    "df_design = pd.read_csv(osp.join(analysis_folder, 'design', 'design_responder_30perc.csv'))\n",
    "assert df_design.shape[0] == significant_connection.size\n",
    "\n",
    "# Responders coded as 1; Non-Responders coded as 0\n",
    "y_true = np.zeros(significant_connection.size)\n",
    "y_true[df_design['responder_30perc_1.0'] == 1] = 1\n",
    "y_true[df_design['responder_30perc_0.0'] == 1] = 0\n",
    "print(np.unique(y_true, return_counts=True))\n",
    "\n",
    "X_connection = significant_connection[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(y, y_pred, **kwargs):\n",
    "    # recall == sensitivity\n",
    "    # recall for other class (0) == specificity\n",
    "    return recall_score(y_true=y, y_pred=y_pred, pos_label=0)\n",
    "\n",
    "\n",
    "def negative_predictive_value(y, y_pred, **kwargs):\n",
    "    # precision == positive predictive value\n",
    "    # precision for other class (0) == negative predictive value\n",
    "    return precision_score(y_true=y, y_pred=y_pred, pos_label=0)\n",
    "\n",
    "\n",
    "def print_results(df_results, analysis_str):\n",
    "    metrics_rename = {'test_ACC': 'Balanced Accuracy',\n",
    "                      'test_SENS': 'Sensitivity',\n",
    "                      'test_SPEC': 'Specificity',\n",
    "                      'test_PPV': 'Positive Predictive Value',\n",
    "                      'test_NPV': 'Negative Predictive Value'}\n",
    "    df_results = df_results[metrics_rename.keys()].copy()\n",
    "    df_results.rename(columns=metrics_rename, inplace=True)\n",
    "    print(f'{analysis_str}')\n",
    "    print()\n",
    "    print('Average performance metrics:')\n",
    "    print(df_results.mean())\n",
    "    print()\n",
    "    print('SD performance metrics:')\n",
    "    print(df_results.std())\n",
    "    \n",
    "    \n",
    "def get_cv(random_state):\n",
    "    # CV structure as in main analysis\n",
    "    return RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "\n",
    "\n",
    "def ttest_func(X, y):\n",
    "    return ttest_ind(X[y == 1], X[y == 0], equal_var=False)\n",
    "\n",
    "\n",
    "def get_classifier(random_state, with_univariate_ftr_selection=False):\n",
    "    if with_univariate_ftr_selection:\n",
    "        # unbiased classifier which includes feature selection on the training set\n",
    "        clf = make_pipeline(SelectFpr(ttest_func, alpha=0.05),\n",
    "                            MinMaxScaler(feature_range=(-1, 1)), \n",
    "                            LinearSVC(random_state=random_state + 1, max_iter=2000, class_weight='balanced'))\n",
    "    else:\n",
    "        # classifier as in main analysis\n",
    "        clf = make_pipeline(MinMaxScaler(feature_range=(-1, 1)), \n",
    "                            LinearSVC(random_state=random_state + 1, max_iter=2000, class_weight='balanced'))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using the group-level finding as feature for classification. This is wrong as this is essentially double dipping, i.e. we take the group-level results (found on the entire sample) and perform classification (i.e. dividing data into training/testing set, etc.) with it. This is leads to a positively biased performance estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  50 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=5)]: Done 250 out of 250 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "# same classification setup as in the main analysis\n",
    "cv = get_cv(random_state)\n",
    "clf = get_classifier(random_state + 1)\n",
    "spec = make_scorer(specificity)\n",
    "npv = make_scorer(negative_predictive_value)\n",
    "scores_incorrect = cross_validate(clf, X_connection, y_true, scoring={'AUC': 'roc_auc',\n",
    "                                                                      'ACC': 'balanced_accuracy',\n",
    "                                                                      'SENS': 'recall', # recall == sensitivity\n",
    "                                                                      'SPEC': spec,\n",
    "                                                                      'PPV': 'precision',\n",
    "                                                                      'NPV': npv},\n",
    "                        cv=cv, n_jobs=5, error_score='raise', verbose=1)\n",
    "df_scores_incorrect = pd.DataFrame(scores_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance via 'double dipping'\n",
      "\n",
      "Average performance metrics:\n",
      "Balanced Accuracy            0.789433\n",
      "Sensitivity                  0.822200\n",
      "Specificity                  0.756667\n",
      "Positive Predictive Value    0.820024\n",
      "Negative Predictive Value    0.819362\n",
      "dtype: float64\n",
      "\n",
      "SD performance metrics:\n",
      "Balanced Accuracy            0.130157\n",
      "Sensitivity                  0.193720\n",
      "Specificity                  0.237494\n",
      "Positive Predictive Value    0.160601\n",
      "Negative Predictive Value    0.193168\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print_results(df_scores_incorrect, \"Performance via 'double dipping'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. This positive bias in performance can also be seen for completely random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done 150 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 250 out of 250 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Alpha cannot be fully FWE corrected via Bonferroni as then we won't significant features here\n",
    "alpha = 0.05 \n",
    "\n",
    "# Create random data\n",
    "X_rand = np.random.rand(40, 1128)\n",
    "y_rand = np.concatenate((np.zeros(19), np.ones(21)))\n",
    "np.random.shuffle(y_rand)\n",
    "\n",
    "# Run t-test on entire sample and select features which are p < alpha (= 0.05)\n",
    "t, p = ttest_ind(X_rand[y_rand == 1], X_rand[y_rand == 0], equal_var=False)\n",
    "X_rand_selected = X_rand[:, p < alpha]\n",
    "\n",
    "# Perform classificaiton only with the selected features\n",
    "cv = get_cv(random_state)\n",
    "clf = get_classifier(random_state + 1)\n",
    "spec = make_scorer(specificity)\n",
    "npv = make_scorer(negative_predictive_value)\n",
    "scores_rand = cross_validate(clf, X_rand_selected, y_rand, scoring={'AUC': 'roc_auc',\n",
    "                                                                    'ACC': 'balanced_accuracy',\n",
    "                                                                    'SENS': 'recall',\n",
    "                                                                    'SPEC': spec,\n",
    "                                                                    'PPV': 'precision',\n",
    "                                                                    'NPV': npv},\n",
    "                        cv=cv, n_jobs=5, error_score='raise', verbose=1)\n",
    "df_scores_rand = pd.DataFrame(scores_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance via 'double dipping' in random data\n",
      "\n",
      "Average performance metrics:\n",
      "Balanced Accuracy            0.999333\n",
      "Sensitivity                  1.000000\n",
      "Specificity                  0.998667\n",
      "Positive Predictive Value    0.999333\n",
      "Negative Predictive Value    1.000000\n",
      "dtype: float64\n",
      "\n",
      "SD performance metrics:\n",
      "Balanced Accuracy            0.010541\n",
      "Sensitivity                  0.000000\n",
      "Specificity                  0.021082\n",
      "Positive Predictive Value    0.010541\n",
      "Negative Predictive Value    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print_results(df_scores_rand, \"Performance via 'double dipping' in random data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Now we will repeat the same procedure but try to make it more correct (i.e. group-level testing within the training set only). We will apply this approach to the real data. Again we cannot use an alpha which FWE corrected since we have only a smaller data set (training set) to work with and cannot use synchronized permutation for FWE correction (but only Bonferroni) as it would be too computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done 150 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 250 out of 250 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Take ALL the correlational features instead of just the group-level selected one\n",
    "X_corr = df_correlations.to_numpy()\n",
    "\n",
    "# Repeat the same classification pipeline with the addition of a t-test for each training set to select features \n",
    "# p < alpha (= 0.05)\n",
    "cv = get_cv(random_state)\n",
    "clf = get_classifier(random_state + 1, with_univariate_ftr_selection=True)\n",
    "\n",
    "spec = make_scorer(specificity)\n",
    "npv = make_scorer(negative_predictive_value)\n",
    "scores_correct = cross_validate(clf, X_corr, y_true, scoring={'AUC': 'roc_auc',\n",
    "                                                              'ACC': 'balanced_accuracy',\n",
    "                                                              'SENS': 'recall',\n",
    "                                                              'SPEC': spec,\n",
    "                                                              'PPV': 'precision',\n",
    "                                                              'NPV': npv},\n",
    "                        cv=cv, n_jobs=5, error_score='raise', verbose=1)\n",
    "df_scores_correct = pd.DataFrame(scores_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbiased performance estimate in real data\n",
      "\n",
      "Average performance metrics:\n",
      "Balanced Accuracy            0.624867\n",
      "Sensitivity                  0.661400\n",
      "Specificity                  0.588333\n",
      "Positive Predictive Value    0.649095\n",
      "Negative Predictive Value    0.641771\n",
      "dtype: float64\n",
      "\n",
      "SD performance metrics:\n",
      "Balanced Accuracy            0.140146\n",
      "Sensitivity                  0.231027\n",
      "Specificity                  0.217236\n",
      "Positive Predictive Value    0.165881\n",
      "Negative Predictive Value    0.216464\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print_results(df_scores_correct, \"Unbiased performance estimate in real data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that the observed performance drops from 78.9% accuracy to 62.5% (balanced) accuracy showing that indeed there was a huge positive bias in the initial ('double dipped') performance estimate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
